{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransducerLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransducerLoss, self).__init__()\n",
    "        \n",
    "    def show_alignment(self, log_alpha):\n",
    "        plt.imshow(log_alpha.cpu().data.numpy().transpose(), origin=\"lower\"); plt.show()\n",
    "    \n",
    "    def compute_log_alpha_and_log_prob(self, encoder_out, decoder_out, y, blank):\n",
    "        \"\"\"\n",
    "        encoder_out: FloatTensor (T, #labels)\n",
    "        decoder_out: FloatTensor (U+1, #labels)\n",
    "        y: LongTensor (U,)\n",
    "        blank: int\n",
    "        \"\"\"\n",
    "        T = len(encoder_out)\n",
    "        U = len(y)\n",
    "        \n",
    "        log_alpha = torch.zeros(T, U+1) #[]\n",
    "        for t in range(T):\n",
    "            \n",
    "            for u in range(U + 1):\n",
    "                if u == 0:\n",
    "                    \n",
    "                    if t == 0:\n",
    "                        log_alpha[t,u] = 0.\n",
    "                    \n",
    "                    else: #t > 0\n",
    "                        null_t_1_0 = encoder_out[t-1, blank] + decoder_out[0, blank]\n",
    "                        log_alpha[t,u] = log_alpha[t-1,u] + null_t_1_0\n",
    "                        \n",
    "                else: #u > 0\n",
    "                    \n",
    "                    if t == 0:\n",
    "                        y_0_u_1 = encoder_out[t, y[u-1]] + decoder_out[u-1, y[u-1]]\n",
    "                        log_alpha[t,u] = log_alpha[t,u-1] + y_0_u_1\n",
    "                    \n",
    "                    else: #t > 0\n",
    "                        y_t_u_1 = encoder_out[t, y[u-1]] + decoder_out[u-1, y[u-1]]\n",
    "                        null_t_1_u = encoder_out[t-1, blank] + decoder_out[u, blank]\n",
    "                        \n",
    "                        log_alpha[t,u] = torch.logsumexp(torch.stack([\n",
    "                            log_alpha[t-1,u] + null_t_1_u,\n",
    "                            log_alpha[t,u-1] + y_t_u_1 \n",
    "                        ]), dim=0)\n",
    "                        \n",
    "                print(log_alpha)\n",
    "        \n",
    "        print(log_alpha[T-1,U])\n",
    "        null_T_1_U = encoder_out[T-1, blank] + decoder_out[U, blank]\n",
    "        log_p_y_x = log_alpha[T-1,U] + null_T_1_U\n",
    "        return log_alpha, log_p_y_x\n",
    "    \n",
    "    def forward(self,encoder_out,decoder_out,targets,input_lengths,target_lengths,reduction=\"none\",blank=0):\n",
    "        \"\"\"\n",
    "        encoder_out: FloatTensor (N, max(input_lengths), #labels)\n",
    "        decoder_out: FloatTensor (N, max(target_lengths)+1, #labels)\n",
    "        targets: LongTensor (N, max(target_lengths))\n",
    "        input_lengths: LongTensor (N)\n",
    "        target_lengths: LongTensor (N)\n",
    "        reduction: \"none\", \"avg\"\n",
    "        blank: int\n",
    "        \"\"\"\n",
    "        batch_size = len(input_lengths)\n",
    "        log_probs = []\n",
    "        for i in range(0, batch_size):\n",
    "            encoder_out_ = encoder_out[i, :input_lengths[i], :]\n",
    "            decoder_out_ = decoder_out[i, :target_lengths[i]+1, :]\n",
    "            y = targets[i, :target_lengths[i]]\n",
    "            log_alpha, log_p_y_x = self.compute_log_alpha_and_log_prob(encoder_out_, decoder_out_, y, blank)\n",
    "            self.show_alignment(log_alpha)\n",
    "            log_probs.append(log_p_y_x)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        return log_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 0]])\n",
      "torch.Size([1, 4, 5])\n",
      "torch.Size([1, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "num_labels = 5\n",
    "blank_index = num_labels-1 # last output = blank\n",
    "batch_size = 1\n",
    "pad = -1\n",
    "T = torch.LongTensor([4])\n",
    "U = torch.LongTensor([3])\n",
    "y = torch.randint(low=0,high=num_labels-1,size=(U[0],)).unsqueeze(0).long()\n",
    "print(y)\n",
    "\n",
    "encoder_out = torch.randn(batch_size, max(T), num_labels).log_softmax(2).detach().requires_grad_()\n",
    "decoder_out = torch.randn(batch_size, max(U)+1, num_labels).log_softmax(2).detach().requires_grad_()\n",
    "print(encoder_out.shape)\n",
    "print(decoder_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[ 0.0000, -2.5336,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000]], grad_fn=<CopySlices>)\n",
      "tensor([[ 0.0000, -2.5336, -5.1332,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000]], grad_fn=<CopySlices>)\n",
      "tensor([[ 0.0000, -2.5336, -5.1332, -8.4604],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000]], grad_fn=<CopySlices>)\n",
      "tensor([[ 0.0000, -2.5336, -5.1332, -8.4604],\n",
      "        [-5.6083,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000]], grad_fn=<CopySlices>)\n",
      "tensor([[ 0.0000, -2.5336, -5.1332, -8.4604],\n",
      "        [-5.6083, -7.5713,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000]], grad_fn=<CopySlices>)\n",
      "tensor([[ 0.0000, -2.5336, -5.1332, -8.4604],\n",
      "        [-5.6083, -7.5713, -9.7141,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000]], grad_fn=<CopySlices>)\n",
      "tensor([[  0.0000,  -2.5336,  -5.1332,  -8.4604],\n",
      "        [ -5.6083,  -7.5713,  -9.7141, -11.6783],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000]], grad_fn=<CopySlices>)\n",
      "tensor([[  0.0000,  -2.5336,  -5.1332,  -8.4604],\n",
      "        [ -5.6083,  -7.5713,  -9.7141, -11.6783],\n",
      "        [ -9.5865,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000]], grad_fn=<CopySlices>)\n",
      "tensor([[  0.0000,  -2.5336,  -5.1332,  -8.4604],\n",
      "        [ -5.6083,  -7.5713,  -9.7141, -11.6783],\n",
      "        [ -9.5865, -11.0247,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000]], grad_fn=<CopySlices>)\n",
      "tensor([[  0.0000,  -2.5336,  -5.1332,  -8.4604],\n",
      "        [ -5.6083,  -7.5713,  -9.7141, -11.6783],\n",
      "        [ -9.5865, -11.0247, -12.7093,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000]], grad_fn=<CopySlices>)\n",
      "tensor([[  0.0000,  -2.5336,  -5.1332,  -8.4604],\n",
      "        [ -5.6083,  -7.5713,  -9.7141, -11.6783],\n",
      "        [ -9.5865, -11.0247, -12.7093, -13.7885],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000]], grad_fn=<CopySlices>)\n",
      "tensor([[  0.0000,  -2.5336,  -5.1332,  -8.4604],\n",
      "        [ -5.6083,  -7.5713,  -9.7141, -11.6783],\n",
      "        [ -9.5865, -11.0247, -12.7093, -13.7885],\n",
      "        [-14.5061,   0.0000,   0.0000,   0.0000]], grad_fn=<CopySlices>)\n",
      "tensor([[  0.0000,  -2.5336,  -5.1332,  -8.4604],\n",
      "        [ -5.6083,  -7.5713,  -9.7141, -11.6783],\n",
      "        [ -9.5865, -11.0247, -12.7093, -13.7885],\n",
      "        [-14.5061, -15.2615,   0.0000,   0.0000]], grad_fn=<CopySlices>)\n",
      "tensor([[  0.0000,  -2.5336,  -5.1332,  -8.4604],\n",
      "        [ -5.6083,  -7.5713,  -9.7141, -11.6783],\n",
      "        [ -9.5865, -11.0247, -12.7093, -13.7885],\n",
      "        [-14.5061, -15.2615, -16.3965,   0.0000]], grad_fn=<CopySlices>)\n",
      "tensor([[  0.0000,  -2.5336,  -5.1332,  -8.4604],\n",
      "        [ -5.6083,  -7.5713,  -9.7141, -11.6783],\n",
      "        [ -9.5865, -11.0247, -12.7093, -13.7885],\n",
      "        [-14.5061, -15.2615, -16.3965, -17.1863]], grad_fn=<CopySlices>)\n",
      "tensor(-17.1863, grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor([[1, 1, 0]])\n",
      "loss: tensor([20.7201], grad_fn=<StackBackward>)\n",
      "encoder_out:\n",
      "tensor([[[-2.4385, -0.9986, -1.4343, -1.4312, -2.7029],\n",
      "         [-1.5404, -2.4761, -2.3677, -1.3246, -1.0729],\n",
      "         [-1.1763, -2.3296, -2.4474, -0.9827, -2.0143],\n",
      "         [-2.0739, -0.7276, -3.6199, -1.4567, -2.0293]]], requires_grad=True)\n",
      "encoder grad:\n",
      "tensor([[[-0.2065, -1.7832,  0.0000,  0.0000, -1.0000],\n",
      "         [-0.3489, -0.1250,  0.0000,  0.0000, -1.0000],\n",
      "         [-0.3307, -0.0496,  0.0000,  0.0000, -1.0000],\n",
      "         [-0.1139, -0.0422,  0.0000,  0.0000, -1.0000]]])\n",
      "decoder_out:\n",
      "tensor([[[-0.4742, -1.5350, -3.3044, -2.6491, -2.9054],\n",
      "         [-1.2321, -1.6010, -0.9235, -3.6830, -2.4730],\n",
      "         [-0.8887, -1.6250, -2.8752, -1.5861, -2.0342],\n",
      "         [-1.3976, -3.3767, -0.7767, -3.3085, -1.5044]]], requires_grad=True)\n",
      "decoder grad:\n",
      "tensor([[[ 0.0000, -1.0000,  0.0000,  0.0000, -0.0570],\n",
      "         [ 0.0000, -1.0000,  0.0000,  0.0000, -0.2368],\n",
      "         [-1.0000,  0.0000,  0.0000,  0.0000, -1.0581],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000, -2.6481]]])\n"
     ]
    }
   ],
   "source": [
    "transducer_loss = TransducerLoss()\n",
    "log_probs = transducer_loss(encoder_out=encoder_out,decoder_out=decoder_out,targets=y,input_lengths=T,target_lengths=U,reduction=\"none\",blank=blank_index)\n",
    "\n",
    "print(\"y:\", y)\n",
    "print(\"loss:\", loss)\n",
    "(-log_probs).mean().backward()\n",
    "\n",
    "print(\"encoder_out:\")\n",
    "print(encoder_out)\n",
    "\n",
    "print(\"encoder grad:\")\n",
    "print(encoder_out.grad)\n",
    "del encoder_out.grad\n",
    "\n",
    "print(\"decoder_out:\")\n",
    "print(decoder_out)\n",
    "\n",
    "print(\"decoder grad:\")\n",
    "print(decoder_out.grad)\n",
    "del decoder_out.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTCModel(\n",
      "  (encoder): Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): ComputeFBANK()\n",
      "      (1): Conv(\n",
      "        (conv): Conv1d(80, 512, kernel_size=(11,), stride=(2,))\n",
      "      )\n",
      "      (2): LeakyReLU(negative_slope=0.125)\n",
      "      (3): GRU(512, 512, batch_first=True, bidirectional=True)\n",
      "      (4): RNNOutputSelect()\n",
      "      (5): Dropout(p=0.2, inplace=False)\n",
      "      (6): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (7): LeakyReLU(negative_slope=0.125)\n",
      "      (8): Downsample()\n",
      "      (9): GRU(512, 512, batch_first=True, bidirectional=True)\n",
      "      (10): RNNOutputSelect()\n",
      "      (11): Dropout(p=0.2, inplace=False)\n",
      "      (12): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (13): LeakyReLU(negative_slope=0.125)\n",
      "      (14): GRU(512, 512, batch_first=True, bidirectional=True)\n",
      "      (15): RNNOutputSelect()\n",
      "      (16): Dropout(p=0.2, inplace=False)\n",
      "      (17): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (18): LeakyReLU(negative_slope=0.125)\n",
      "      (19): Linear(in_features=512, out_features=1001, bias=True)\n",
      "      (20): LogSoftmax()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[26, 464, 33, 86, 115, 8, 86, 10, 9, 14, 3, 55, 20, 5, 379, 7, 81, 8, 10, 358, 8]\n",
      "HE COMPUTER COULDETERNALS ON IN THE KITCHENPLEE\n",
      "[26, 23, 464, 33, 86, 115, 38, 549, 5, 368, 3, 55, 20, 5, 379, 7, 81, 8, 10, 779]\n",
      "HEY COMPUTER COULD YOU TURN THE LIGHTS ON IN THE KITCHEN PLEASE\n",
      "torch.Size([1, 90, 1001])\n",
      "torch.Size([1, 21, 1001])\n",
      "tensor([[ 26,  23, 464,  33,  86, 115,  38, 549,   5, 368,   3,  55,  20,   5,\n",
      "         379,   7,  81,   8,  10, 779]])\n",
      "tensor([20])\n"
     ]
    }
   ],
   "source": [
    "# use pre-trained CTC model\n",
    "from models import CTCModel\n",
    "from data import read_config\n",
    "import sentencepiece as spm\n",
    "import soundfile as sf\n",
    "\n",
    "config = read_config(\"experiments/80_mel.cfg\")\n",
    "model = CTCModel(config=config).eval()\n",
    "model.load_pretrained(\"experiments/80_mel/training/best_model.pth\")\n",
    "print(model)\n",
    "\n",
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.Load(\"tokenizer_1000_tokens.model\")\n",
    "\n",
    "x,fs = sf.read(\"../end-to-end-SLU/test.wav\")\n",
    "x = torch.tensor(x).unsqueeze(0).float()\n",
    "\n",
    "guess = model.infer(x)[0]\n",
    "truth = \"HEY COMPUTER COULD YOU TURN THE LIGHTS ON IN THE KITCHEN PLEASE\"\n",
    "print(guess)\n",
    "print(tokenizer.DecodeIds(guess))\n",
    "print(tokenizer.EncodeAsIds(truth))\n",
    "print(truth)\n",
    "\n",
    "encoder_out = model.encoder.forward(x, T=None).detach().requires_grad_()\n",
    "print(encoder_out.shape)\n",
    "num_labels = encoder_out.shape[2]\n",
    "blank_index = num_labels-1\n",
    "T = torch.LongTensor([encoder_out.shape[1]])\n",
    "U = torch.LongTensor([ len(tokenizer.EncodeAsIds(truth)) ]) #torch.LongTensor([ len(guess) ])#\n",
    "y = torch.LongTensor([ tokenizer.EncodeAsIds(truth) ]) #torch.LongTensor([ guess ]) #\n",
    "decoder_out = torch.ones(batch_size, max(U)+1, num_labels).log_softmax(2).detach().requires_grad_()\n",
    "print(decoder_out.shape)\n",
    "print(y)\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8594)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.randn(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
