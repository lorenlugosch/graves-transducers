{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransducerLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransducerLoss, self).__init__()\n",
    "        \n",
    "    def show_alignment(self, log_alpha):\n",
    "        plt.imshow(log_alpha.cpu().data.numpy().transpose()); plt.show()\n",
    "    \n",
    "    def compute_log_alpha_and_log_prob(self, encoder_out, decoder_out, y, blank):\n",
    "        \"\"\"\n",
    "        encoder_out: FloatTensor (T, #labels)\n",
    "        decoder_out: FloatTensor (U+1, #labels)\n",
    "        y: LongTensor (U,)\n",
    "        blank: int\n",
    "        \"\"\"\n",
    "        T = len(encoder_out)\n",
    "        U = len(y)\n",
    "        \n",
    "        log_alphas = []\n",
    "        for t in range(T):\n",
    "            log_alpha_t = torch.zeros(U+1)\n",
    "            \n",
    "            for u in range(U + 1):\n",
    "                if u == 0:\n",
    "                    if t == 0:\n",
    "                        log_alpha_t[u] = 0.\n",
    "                    else: #t > 0\n",
    "                        null_t_1_0 = encoder_out[t-1, blank] + decoder_out[0, blank]\n",
    "                        log_alpha_t[u] = log_alpha_t_1[0] + null_t_1_0\n",
    "                        \n",
    "                else: #u > 0\n",
    "                    if t == 0:\n",
    "                        y_0_u_1 = encoder_out[0, y[u-1]] + decoder_out[u-1, y[u-1]]\n",
    "                        log_alpha_t[u] = log_alpha_t[u-1] + y_0_u_1\n",
    "                    else: #t > 0\n",
    "                        y_t_u_1 = encoder_out[t, y[u-1]] + decoder_out[u-1, y[u-1]]\n",
    "                        null_t_1_u = encoder_out[t-1, blank] + decoder_out[u, blank]\n",
    "                        \n",
    "                        log_alpha_t[u] = torch.logsumexp(torch.tensor([\n",
    "                            log_alpha_t_1[u] + null_t_1_u,\n",
    "                            log_alpha_t[u-1] + y_t_u_1 \n",
    "                        ]), dim=0)\n",
    "                        print(\"grad?\", log_alpha_t[u].requires_grad)\n",
    "                                \n",
    "            log_alphas.append(log_alpha_t)\n",
    "            log_alpha_t_1 = log_alphas[-1]\n",
    "            #print(log_alphas[-1])\n",
    "            \n",
    "        log_alpha = torch.stack(log_alphas)\n",
    "        null_T_1_U = encoder_out[T-1, blank] + decoder_out[U, blank]\n",
    "        log_p_y_x = log_alpha[T-1,U] + null_T_1_U\n",
    "        return log_alpha, log_p_y_x\n",
    "    \n",
    "    def forward(self,encoder_out,decoder_out,targets,input_lengths,target_lengths,reduction=\"none\",blank=0):\n",
    "        \"\"\"\n",
    "        encoder_out: FloatTensor (N, max(input_lengths), #labels)\n",
    "        decoder_out: FloatTensor (N, max(target_lengths)+1, #labels)\n",
    "        targets: LongTensor (N, max(target_lengths))\n",
    "        input_lengths: LongTensor (N)\n",
    "        target_lengths: LongTensor (N)\n",
    "        reduction: \"none\", \"avg\"\n",
    "        blank: int\n",
    "        \"\"\"\n",
    "        batch_size = len(input_lengths)\n",
    "        losses = []\n",
    "        for i in range(0, batch_size):\n",
    "            encoder_out_ = encoder_out[i, :input_lengths[i], :]\n",
    "            decoder_out_ = decoder_out[i, :target_lengths[i]+1, :]\n",
    "            y = targets[i, :target_lengths[i]]\n",
    "            log_alpha, log_p_y_x = self.compute_log_alpha_and_log_prob(encoder_out_, decoder_out_, y, blank)\n",
    "            self.show_alignment(log_alpha)\n",
    "            loss = -log_p_y_x # TODO \n",
    "            losses.append(loss)\n",
    "        losses = torch.stack(losses)\n",
    "        if reduction==\"none\": return losses\n",
    "        if reduction==\"avg\": return losses.mean()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 0]])\n",
      "torch.Size([1, 4, 5])\n",
      "torch.Size([1, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "num_labels = 5\n",
    "blank_index = num_labels-1 # last output = blank\n",
    "batch_size = 1\n",
    "pad = -1\n",
    "T = torch.LongTensor([4])\n",
    "U = torch.LongTensor([3])\n",
    "y = torch.randint(low=0,high=num_labels-1,size=(U[0],)).unsqueeze(0).long()\n",
    "print(y)\n",
    "\n",
    "encoder_out = torch.randn(batch_size, max(T), num_labels).log_softmax(2).detach().requires_grad_()\n",
    "decoder_out = torch.randn(batch_size, max(U)+1, num_labels).log_softmax(2).detach().requires_grad_()\n",
    "print(encoder_out.shape)\n",
    "print(decoder_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTCModel(\n",
      "  (encoder): Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): ComputeFBANK()\n",
      "      (1): Conv(\n",
      "        (conv): Conv1d(80, 512, kernel_size=(11,), stride=(2,))\n",
      "      )\n",
      "      (2): LeakyReLU(negative_slope=0.125)\n",
      "      (3): GRU(512, 512, batch_first=True, bidirectional=True)\n",
      "      (4): RNNOutputSelect()\n",
      "      (5): Dropout(p=0.2, inplace=False)\n",
      "      (6): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (7): LeakyReLU(negative_slope=0.125)\n",
      "      (8): Downsample()\n",
      "      (9): GRU(512, 512, batch_first=True, bidirectional=True)\n",
      "      (10): RNNOutputSelect()\n",
      "      (11): Dropout(p=0.2, inplace=False)\n",
      "      (12): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (13): LeakyReLU(negative_slope=0.125)\n",
      "      (14): GRU(512, 512, batch_first=True, bidirectional=True)\n",
      "      (15): RNNOutputSelect()\n",
      "      (16): Dropout(p=0.2, inplace=False)\n",
      "      (17): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (18): LeakyReLU(negative_slope=0.125)\n",
      "      (19): Linear(in_features=512, out_features=1001, bias=True)\n",
      "      (20): LogSoftmax()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[26, 464, 33, 86, 115, 8, 86, 10, 9, 14, 3, 55, 20, 5, 379, 7, 81, 8, 10, 358, 8]\n",
      "HE COMPUTER COULDETERNALS ON IN THE KITCHENPLEE\n",
      "[26, 23, 464, 33, 86, 115, 38, 549, 5, 368, 3, 55, 20, 5, 379, 7, 81, 8, 10, 779]\n",
      "HEY COMPUTER COULD YOU TURN THE LIGHTS ON IN THE KITCHEN PLEASE\n",
      "torch.Size([1, 90, 1001])\n",
      "torch.Size([1, 21, 1001])\n",
      "tensor([[ 26,  23, 464,  33,  86, 115,  38, 549,   5, 368,   3,  55,  20,   5,\n",
      "         379,   7,  81,   8,  10, 779]])\n",
      "tensor([20])\n"
     ]
    }
   ],
   "source": [
    "# use pre-trained CTC model\n",
    "from models import CTCModel\n",
    "from data import read_config\n",
    "import sentencepiece as spm\n",
    "import soundfile as sf\n",
    "\n",
    "config = read_config(\"experiments/80_mel.cfg\")\n",
    "model = CTCModel(config=config).eval()\n",
    "model.load_pretrained(\"experiments/80_mel/training/best_model.pth\")\n",
    "print(model)\n",
    "\n",
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.Load(\"tokenizer_1000_tokens.model\")\n",
    "\n",
    "x,fs = sf.read(\"../end-to-end-SLU/test.wav\")\n",
    "x = torch.tensor(x).unsqueeze(0).float()\n",
    "\n",
    "guess = model.infer(x)[0]\n",
    "truth = \"HEY COMPUTER COULD YOU TURN THE LIGHTS ON IN THE KITCHEN PLEASE\"\n",
    "print(guess)\n",
    "print(tokenizer.DecodeIds(guess))\n",
    "print(tokenizer.EncodeAsIds(truth))\n",
    "print(truth)\n",
    "\n",
    "encoder_out = model.encoder.forward(x, T=None).detach().requires_grad_()\n",
    "print(encoder_out.shape)\n",
    "num_labels = encoder_out.shape[2]\n",
    "blank_index = num_labels-1\n",
    "T = torch.LongTensor([encoder_out.shape[1]])\n",
    "U = torch.LongTensor([ len(tokenizer.EncodeAsIds(truth)) ]) #torch.LongTensor([ len(guess) ])#\n",
    "y = torch.LongTensor([ tokenizer.EncodeAsIds(truth) ]) #torch.LongTensor([ guess ]) #\n",
    "decoder_out = torch.ones(batch_size, max(U)+1, num_labels).log_softmax(2).detach().requires_grad_()\n",
    "print(decoder_out.shape)\n",
    "print(y)\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad? True\n",
      "grad? True\n",
      "grad? True\n",
      "grad? True\n",
      "grad? True\n",
      "grad? True\n",
      "grad? True\n",
      "grad? True\n",
      "grad? True\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor([17.1863], grad_fn=<StackBackward>)\n",
      "encoder_out:\n",
      "tensor([[[-2.4385, -0.9986, -1.4343, -1.4312, -2.7029],\n",
      "         [-1.5404, -2.4761, -2.3677, -1.3246, -1.0729],\n",
      "         [-1.1763, -2.3296, -2.4474, -0.9827, -2.0143],\n",
      "         [-2.0739, -0.7276, -3.6199, -1.4567, -2.0293]]], requires_grad=True)\n",
      "encoder grad:\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "decoder_out:\n",
      "tensor([[[-0.4742, -1.5350, -3.3044, -2.6491, -2.9054],\n",
      "         [-1.2321, -1.6010, -0.9235, -3.6830, -2.4730],\n",
      "         [-0.8887, -1.6250, -2.8752, -1.5861, -2.0342],\n",
      "         [-1.3976, -3.3767, -0.7767, -3.3085, -1.5044]]], requires_grad=True)\n",
      "decoder grad:\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "transducer_loss = TransducerLoss()\n",
    "loss = transducer_loss(encoder_out=encoder_out,decoder_out=decoder_out,targets=y,input_lengths=T,target_lengths=U,reduction=\"none\",blank=blank_index)\n",
    "\n",
    "print(\"loss:\", loss)\n",
    "loss.mean().backward()\n",
    "\n",
    "print(\"encoder_out:\")\n",
    "print(encoder_out)\n",
    "\n",
    "print(\"encoder grad:\")\n",
    "print(encoder_out.grad)\n",
    "del encoder_out.grad\n",
    "\n",
    "print(\"decoder_out:\")\n",
    "print(decoder_out)\n",
    "\n",
    "print(\"decoder grad:\")\n",
    "print(decoder_out.grad)\n",
    "del decoder_out.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.1759"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-2.7029-2.473"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
