{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransducerLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransducerLoss, self).__init__()\n",
    "        \n",
    "    def show_alignment(self, log_alpha):\n",
    "        plt.imshow(log_alpha.cpu().data.numpy().transpose()); plt.show()\n",
    "    \n",
    "    def compute_log_alpha_and_log_prob(self, encoder_out, decoder_out, y, blank):\n",
    "        \"\"\"\n",
    "        encoder_out: FloatTensor (T, #labels)\n",
    "        decoder_out: FloatTensor (U, #labels)\n",
    "        y: LongTensor (U,)\n",
    "        blank: int\n",
    "        \"\"\"\n",
    "        T = len(encoder_out)\n",
    "        U = len(y)\n",
    "        \n",
    "        log_alphas = []\n",
    "        log_alpha_t_1 = torch.zeros(U)\n",
    "        for t in range(T):\n",
    "            log_alpha_t = [] #torch.zeros(U)\n",
    "            \n",
    "            for u in range(U):\n",
    "                # assume encoder and decoder output a logsoftmax over labels\n",
    "                y_t_u = encoder_out[t, y[u]] + decoder_out[u, y[u]]\n",
    "                null_t_u = encoder_out[t, blank] + decoder_out[u, blank]\n",
    "                if u == 0: log_alpha_t_u_1 = 0\n",
    "                else: log_alpha_t_u_1 = log_alpha_t[u-1]\n",
    "                log_alpha_t_1_u = log_alpha_t_1[u]\n",
    "                \n",
    "                # if we wanted to use a \"joiner\" network, we would use these lines instead\n",
    "                #y_t_u = self.joiner(encoder_out[t, :], decoder_out[u, :])[y[u]]\n",
    "                #null_t_u = self.joiner(encoder_out[t, :], decoder_out[u, :])[blank]\n",
    "            \n",
    "                # log_alpha_t[u] = torch.logsumexp(torch.tensor([\n",
    "                log_alpha_t_u = torch.logsumexp(torch.tensor([\n",
    "                    log_alpha_t_1_u + y_t_u,\n",
    "                    log_alpha_t_u_1 + null_t_u \n",
    "                ]), dim=0)\n",
    "                \n",
    "                log_alpha_t.append(log_alpha_t_u)\n",
    "            \n",
    "            log_alpha_t = torch.tensor(log_alpha_t)\n",
    "            log_alphas.append(log_alpha_t)\n",
    "            log_alpha_t_1 = log_alphas[-1]\n",
    "            print(log_alpha_t_1)\n",
    "            #log_alpha = torch.stack(log_alphas); self.show_alignment(log_alpha)\n",
    "            \n",
    "        log_alpha = torch.stack(log_alphas)\n",
    "        null_T_U = encoder_out[T-1, blank] + decoder_out[U-1, blank]\n",
    "        log_p_y_x = log_alpha[T-1,U-1] + null_T_U \n",
    "        return log_alpha, log_p_y_x\n",
    "    \n",
    "    def forward(self,encoder_out,decoder_out,targets,input_lengths,target_lengths,reduction=\"none\",blank=0):\n",
    "        \"\"\"\n",
    "        encoder_out: FloatTensor (N, max(input_lengths), #labels)\n",
    "        decoder_out: FloatTensor (N, max(target_lengths), #labels)\n",
    "        targets: LongTensor (N, max(target_lengths))\n",
    "        input_lengths: LongTensor (N)\n",
    "        target_lengths: LongTensor (N)\n",
    "        reduction: \"none\", \"avg\"\n",
    "        blank: int\n",
    "        \"\"\"\n",
    "        batch_size = len(input_lengths)\n",
    "        losses = []\n",
    "        for i in range(0, batch_size):\n",
    "            encoder_out_ = encoder_out[i, :input_lengths[i], :]\n",
    "            decoder_out_ = decoder_out[i, :target_lengths[i], :]\n",
    "            y = targets[i, :target_lengths[i]]\n",
    "            log_alpha, log_p_y_x = self.compute_log_alpha_and_log_prob(encoder_out_, decoder_out_, y, blank)\n",
    "            self.show_alignment(log_alpha)\n",
    "            loss = -log_p_y_x # TODO \n",
    "            losses.append(loss)\n",
    "        losses = torch.stack(losses)\n",
    "        if reduction==\"none\": return losses\n",
    "        if reduction==\"avg\": return losses.mean()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 3, 1, 0, 0, 1, 1, 1]])\n",
      "torch.Size([1, 20, 5])\n",
      "torch.Size([1, 8, 5])\n"
     ]
    }
   ],
   "source": [
    "num_labels = 5\n",
    "blank_index = num_labels-1 # last output = blank\n",
    "batch_size = 1\n",
    "pad = -1\n",
    "T = torch.LongTensor([20])\n",
    "U = torch.LongTensor([8])\n",
    "y = torch.randint(low=0,high=num_labels-1,size=(U[0],)).unsqueeze(0).long()\n",
    "print(y)\n",
    "\n",
    "encoder_out = torch.randn(batch_size, max(T), num_labels).log_softmax(2).detach().requires_grad_()\n",
    "decoder_out = torch.randn(batch_size, max(U), num_labels).log_softmax(2).detach().requires_grad_()\n",
    "print(encoder_out.shape)\n",
    "print(decoder_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTCModel(\n",
      "  (encoder): Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): ComputeFBANK()\n",
      "      (1): Conv(\n",
      "        (conv): Conv1d(80, 512, kernel_size=(11,), stride=(2,))\n",
      "      )\n",
      "      (2): LeakyReLU(negative_slope=0.125)\n",
      "      (3): GRU(512, 512, batch_first=True, bidirectional=True)\n",
      "      (4): RNNOutputSelect()\n",
      "      (5): Dropout(p=0.2, inplace=False)\n",
      "      (6): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (7): LeakyReLU(negative_slope=0.125)\n",
      "      (8): Downsample()\n",
      "      (9): GRU(512, 512, batch_first=True, bidirectional=True)\n",
      "      (10): RNNOutputSelect()\n",
      "      (11): Dropout(p=0.2, inplace=False)\n",
      "      (12): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (13): LeakyReLU(negative_slope=0.125)\n",
      "      (14): GRU(512, 512, batch_first=True, bidirectional=True)\n",
      "      (15): RNNOutputSelect()\n",
      "      (16): Dropout(p=0.2, inplace=False)\n",
      "      (17): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (18): LeakyReLU(negative_slope=0.125)\n",
      "      (19): Linear(in_features=512, out_features=1001, bias=True)\n",
      "      (20): LogSoftmax()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[26, 464, 33, 86, 115, 8, 86, 10, 9, 14, 3, 55, 20, 5, 379, 7, 81, 8, 10, 358, 8]\n",
      "HE COMPUTER COULDETERNALS ON IN THE KITCHENPLEE\n",
      "[26, 23, 464, 33, 86, 115, 38, 549, 5, 368, 3, 55, 20, 5, 379, 7, 81, 8, 10, 779]\n",
      "HEY COMPUTER COULD YOU TURN THE LIGHTS ON IN THE KITCHEN PLEASE\n",
      "torch.Size([1, 90, 1001])\n",
      "torch.Size([1, 20, 1001])\n",
      "tensor(-11.2424, grad_fn=<MinBackward1>)\n",
      "tensor(-3.9102, grad_fn=<MaxBackward1>)\n",
      "tensor([[ 26,  23, 464,  33,  86, 115,  38, 549,   5, 368,   3,  55,  20,   5,\n",
      "         379,   7,  81,   8,  10, 779]])\n"
     ]
    }
   ],
   "source": [
    "# use pre-trained CTC model\n",
    "from models import CTCModel\n",
    "from data import read_config\n",
    "import sentencepiece as spm\n",
    "import soundfile as sf\n",
    "\n",
    "config = read_config(\"experiments/80_mel.cfg\")\n",
    "model = CTCModel(config=config).eval()\n",
    "model.load_pretrained(\"experiments/80_mel/training/best_model.pth\")\n",
    "print(model)\n",
    "\n",
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.Load(\"tokenizer_1000_tokens.model\")\n",
    "\n",
    "x,fs = sf.read(\"../end-to-end-SLU/test.wav\")\n",
    "x = torch.tensor(x).unsqueeze(0).float()\n",
    "\n",
    "guess = model.infer(x)[0]\n",
    "truth = \"HEY COMPUTER COULD YOU TURN THE LIGHTS ON IN THE KITCHEN PLEASE\"\n",
    "print(guess)\n",
    "print(tokenizer.DecodeIds(guess))\n",
    "print(tokenizer.EncodeAsIds(truth))\n",
    "print(truth)\n",
    "\n",
    "encoder_out = model.encoder.forward(x, T=None).detach().requires_grad_()\n",
    "print(encoder_out.shape)\n",
    "num_labels = encoder_out.shape[2]\n",
    "blank_index = num_labels-1\n",
    "T = torch.LongTensor([encoder_out.shape[1]])\n",
    "U = torch.LongTensor([ len(tokenizer.EncodeAsIds(truth)) ]) #torch.LongTensor([ len(guess) ])#\n",
    "y = torch.LongTensor([ tokenizer.EncodeAsIds(truth) ]) #torch.LongTensor([ guess ]) #\n",
    "decoder_out = torch.randn(batch_size, max(U), num_labels).log_softmax(2).detach().requires_grad_()\n",
    "print(decoder_out.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.9554, -3.2427, -2.7443, -4.8041, -3.2075, -2.9009, -2.9757, -1.6584])\n",
      "tensor([-5.3821, -6.1220, -6.0725, -8.9134, -5.6564, -6.3831, -6.5767, -3.9035])\n",
      "tensor([ -5.7514,  -9.1779,  -8.4534, -13.8421, -11.3268,  -8.9239,  -9.2291,\n",
      "         -5.1995])\n",
      "tensor([ -3.8964,  -6.1104,  -9.7250, -13.2625, -15.8497, -12.5857, -12.8887,\n",
      "         -7.6169])\n",
      "tensor([ -5.2493,  -8.2158, -12.7289, -17.4103, -19.5887, -16.5191, -16.9178,\n",
      "        -10.3055])\n",
      "tensor([ -6.4031, -10.0844, -14.9984, -21.0889, -23.6496, -19.2034, -19.7208,\n",
      "        -11.7448])\n",
      "tensor([ -8.6419, -12.2785, -19.2489, -26.7645, -28.1458, -23.8466, -24.4919,\n",
      "        -15.1430])\n",
      "tensor([ -5.2521,  -8.8242, -13.9053, -18.7961, -23.0533, -27.7018, -29.3789,\n",
      "        -18.7910])\n",
      "tensor([ -5.0760,  -8.3911, -13.2947, -18.0282, -22.1234, -27.0360, -30.5255,\n",
      "        -20.9413])\n",
      "tensor([ -6.5709, -10.3085, -15.5576, -21.8388, -25.8684, -29.6829, -33.1823,\n",
      "        -22.4040])\n",
      "tensor([ -5.8585, -10.0304, -15.6868, -21.1870, -25.9639, -31.4655, -35.5042,\n",
      "        -24.6435])\n",
      "tensor([ -3.4116,  -5.1432,  -8.3838, -11.4342, -13.8522, -17.0976, -18.9999,\n",
      "        -21.4832])\n",
      "tensor([ -5.6928,  -9.2364, -12.7076, -15.9914, -16.7697, -21.2912, -23.5873,\n",
      "        -24.8204])\n",
      "tensor([ -4.7264,  -7.7626, -12.3127, -16.6591, -19.1933, -23.6474, -26.6620,\n",
      "        -28.1067])\n",
      "tensor([ -4.8132,  -7.9722, -12.5743, -17.0535, -20.6668, -25.1076, -28.2151,\n",
      "        -29.7474])\n",
      "tensor([ -5.3102,  -8.9910, -13.8836, -18.8177, -22.4635, -27.0750, -30.3289,\n",
      "        -31.2243])\n",
      "tensor([ -5.5094,  -9.3107, -14.6347, -19.7819, -24.1962, -29.4488, -33.2956,\n",
      "        -34.6826])\n",
      "tensor([ -3.8560,  -6.0329,  -9.7198, -13.2166, -16.0810, -19.7728, -22.1216,\n",
      "        -25.0518])\n",
      "tensor([ -4.5325,  -7.0949, -11.3424, -15.5355, -18.9879, -22.7698, -25.2265,\n",
      "        -27.3348])\n",
      "tensor([ -6.0582,  -9.2793, -14.3599, -19.0178, -21.0683, -26.0259, -28.8760,\n",
      "        -29.7804])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor([34.9952], grad_fn=<StackBackward>)\n",
      "encoder grad:\n",
      "tensor([[[ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0., -1.]]])\n",
      "decoder grad:\n",
      "tensor([[[ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0., -1.]]])\n"
     ]
    }
   ],
   "source": [
    "transducer_loss = TransducerLoss()\n",
    "loss = transducer_loss(encoder_out=encoder_out,decoder_out=decoder_out,targets=y,input_lengths=T,target_lengths=U,reduction=\"none\",blank=blank_index)\n",
    "\n",
    "print(\"loss:\", loss)\n",
    "loss.mean().backward()\n",
    "\n",
    "print(\"encoder grad:\")\n",
    "print(encoder_out.grad)\n",
    "del encoder_out.grad\n",
    "\n",
    "print(\"decoder grad:\")\n",
    "print(decoder_out.grad)\n",
    "del decoder_out.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
