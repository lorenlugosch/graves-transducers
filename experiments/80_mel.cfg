[experiment]
seed=1
folder=experiments/80_mel

[model]
num_tokens=100
num_encoder_layers=2
num_encoder_hidden=512
num_decoder_layers=2
num_decoder_hidden=128
num_mel_bins=80
bidirectional=True
normalize_fbank=False
tokenizer_training_text_path=librispeech-lm-norm.txt

[training]
base_path=/home/lugosch/data/LibriSpeech
lr=0.0005
lr_period=1
gamma=0.9
batch_size=16
num_epochs=100

[inference]
beam_width=10
